---
title: "Assignment Practical Machine Learning"
author: "Eduardo Rosas Navarro"
date: '2025-03-11'
output:
  html_document: default
  pdf_document: default
lang: es-ES
toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\pagebreak
# Exploratory and preprocessing
## Import of data

```{r echo=T, message=F, warning=F}
data_train <- read.csv("./data/pml-training.csv")
dim(data_train)
```

## Data cleaning and validation
The following code is responsible for ensuring data integrity by identifying and handling blank spaces in the dataset. This step is crucial in data preprocessing to prevent potential issues in further analysis and machine learning applications. Many algorithms do not handle blank values well, and they can lead to incorrect assumptions if not treated properly.

The first step in the validation process is to check if there are any blank spaces in the dataset. The code retrieves the first value from the "kurtosis_roll_belt" column and performs two checks:
1. It verifies whether the value is NA (missing).
2. It checks if the value consists of only blank spaces.
These checks are essential to determine the presence of inconsistent data that needs to be corrected.
```{r echo=T, message=F, warning=F}
# Check if there are blank spaces in some columns
first_value <- data_train$kurtosis_roll_belt[1]
is_na <- is.na(first_value)  # Check if it is NA
is_space <- trimws(first_value) == ""  # Check if it is a blank space

# The results of these checks help in determining whether further data cleaning is required.
is_na
is_space
```

### Drop spaces
The next step involves systematically replacing blank spaces in the dataset with NA values. Many times, blank spaces are not automatically recognized as missing values (NA), leading to  data inconsistencies. By replacing them explicitly, it ensures a uniform structure across the dataset.
```{r echo=T}
# Drop spaces
data_train <- data.frame(lapply(data_train, function(x) {
  if (is.character(x)) {
    x[trimws(x) == ""] <- NA
  } else if (is.factor(x)) {
    # Convert to character to handle blank spaces
    x_char <- as.character(x)
    x_char[trimws(x_char) == ""] <- NA
    # Optional: Convert back to factor
    x <- factor(x_char)
  }
  return(x)
}))
```

## Fill rate analysis
This section analyzes the completeness of the dataset by calculating the fill rate of each column. The fill rate represents the percentage of non-missing values in a given column. A high fill rate indicates a well-populated variable, whereas a low fill rate suggests a high percentage of missing values, which may affect model performance if not handled properly.

```{r echo=T}
fill_rate <- colMeans(!is.na(data_train)) * 100
fill_rate_df <- data.frame(Column = names(fill_rate), FillRate = fill_rate)
head(fill_rate_df, 20)
```


### Individual verification of fill rates
```{r echo=T}
non_null_count <- sum(!is.na(data_train$kurtosis_roll_belt))
non_null_count
```

### Drop features with fill rate <= 5
To improve dataset quality, columns with a fill rate of 5% or less are removed. Features with excessive missing values provide little useful information and can negatively impact model training and performance.
```{r echo=T, warning=F, message=F}
vars_to_remove <- fill_rate_df$Column[fill_rate_df$FillRate <= 5]
data_train <- data_train[, !(names(data_train) %in% vars_to_remove)]
```
By filtering out low-quality features, the dataset retains only the most relevant variables. This step enhances data reliability, improves computational efficiency, and helps avoid issues caused by excessive missing values in machine learning models.


### Count of classes
```{r echo=T, warning=F, message=F}
data_train$classe <- as.factor(data_train$classe)
library(dplyr)
data_train %>%
  count(classe)
```

# Modeling data
## Split sample
To evaluate the model's performance, the dataset is split into a training set (75%) and a test set (25%). The training set is used to build the model, while the test set is used for final evaluation.
```{r echo=T, warning=F, message=F}
set.seed(100)
trainIndex <- sample(seq_len(nrow(data_train)), size = 0.75 * nrow(data_train))
train_data <- data_train[trainIndex, ]
test_data <- data_train[-trainIndex, ]

dim(train_data)
dim(test_data)
```

## Cross Validation
Cross-validation is performed to estimate the model's generalization ability.
A 5-fold cross-validation is used, meaning the training set is divided into 5 parts, where each part is used as a validation set once while the remaining 4 parts train the model.
```{r echo=T, warning=F, message=F}
library(randomForest)
# Number of folds for cross-validation
k <- 5
set.seed(100)  # For reproducibility

# Create fold assignments for each observation in train_data
folds <- sample(rep(1:k, length.out = nrow(train_data)))

# Initialize vector to store accuracy for each fold
cv_accuracy <- numeric(k)

for(i in 1:k) {
  # Split the data: use all folds except the i-th one for training
  cv_train <- train_data[folds != i, ]
  cv_valid <- train_data[folds == i, ]
  
  # Train the Random Forest model
  rf_model <- randomForest(classe ~ ., data = cv_train, ntree = 100)
  
  # Predict on the validation set
  preds <- predict(rf_model, cv_valid)
  
  # Calculate accuracy for this fold
  cm <- table(Predicted = preds, Actual = cv_valid$classe)
  cv_accuracy[i] <- sum(diag(cm)) / sum(cm)
  
  cat("Fold", i, "- Accuracy:", round(cv_accuracy[i], 4), "\n")
}
```

### Accuracy average
Compute the average accuracy across all folds.
This metric provides an estimate of how well the model is expected to perform on unseen data.
```{r echo=T, warning=F, message=F}
# Average accuracy in cross-validation
mean_accuracy <- mean(cv_accuracy)
cat("Average cross-validation accuracy:", round(mean_accuracy, 4), "\n\n")
```

## Adjusted model
Once cross-validation confirms the model's performance, a final model is trained using the entire training set. This model is then evaluated on the test set.
```{r echo=T, warning=F, message=F}
# Train the final model using the entire train_data
final_model <- randomForest(classe ~ ., data = train_data, ntree = 500)
final_predictions <- predict(final_model, test_data)

# Calculate accuracy on the test set
cm_final <- table(Predicted = final_predictions, Actual = test_data$classe)
final_accuracy <- sum(diag(cm_final)) / sum(cm_final)
cat("Accuracy on the test set:", round(final_accuracy, 4), "\n")
```

<!-- # Quiz -->
<!-- ```{r echo=T, warning=F, message=F} -->
<!-- data_quiz <- read.csv("./data/pml-testing.csv") -->

<!-- # Ensure the response variable in train_data is a factor -->
<!-- train_data$classe <- as.factor(train_data$classe) -->

<!-- # Get the names of the variables used in the model (excluding the response variable) -->
<!-- training_vars <- setdiff(names(train_data), "classe") -->

<!-- # Filter `data_quiz` to have only the same variables -->
<!-- filtered_data_quiz <- data_quiz[, training_vars, drop = FALSE] -->

<!-- # Predict on `data_quiz` -->
<!-- data_quiz$classe <- predict(final_model, newdata = filtered_data_quiz) -->
<!-- ``` -->

<!-- ```{r echo=T, warning=F, message=F} -->
<!-- data_quiz %>% -->
<!--   count(classe) -->
<!-- ``` -->



